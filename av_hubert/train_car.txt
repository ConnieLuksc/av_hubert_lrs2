cd /files1/connie/av_hubert_lrs2/av_hubert
export PYTHONPATH=$PYTHONPATH:/files1/connie/av_hubert_lrs2/av_hubert/fairseq

CUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-hydra-train \
    --config-dir avhubert/conf/finetune \
    --config-name large_lrs3_433h.yaml \
    task.data=/files1/connie/av_hubert_lrs2_whole/433h_data \
    task.label_dir=/files1/connie/av_hubert_lrs2_whole/433h_data \
    task.tokenizer_bpe_model=/files1/connie/av_hubert_lrs2_whole/spm1000/spm_unigram1000.model \
    model.w2v_path=/files1/connie/avhubert_model/large_lrs3_iter5.pt \
    task.modalities="['audio','video']" \
    task.pad_audio=true \
    task.max_sample_size=500 \
    task.stack_order_audio=4 \
    model.dropout=0.1 \
    model.attention_dropout=0.1 \
    model.decoder_dropout=0.1 \
    model.decoder_attention_dropout=0.1 \
    model.feature_grad_mult=0.1 \
    hydra.run.dir=/files1/connie/av_hubert_lrs2_whole/car/car_4_4gpu\
    common.user_dir=/files1/connie/av_hubert_lrs2/av_hubert/avhubert_car \
    distributed_training.distributed_world_size=4 \
    distributed_training.nprocs_per_node=4 \
    distributed_training.distributed_init_method=tcp://localhost:10003 \
    optimization.lr=[0.00005] \
    optimization.update_freq=[10] \
    optimization.max_update=200000 \
    optimization.clip_norm=1.0 \
    dataset.max_tokens=800